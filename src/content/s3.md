---
slug: s3
title: Simple Storage Service (S3)
topic: Storage
---
### Buckets
* storing objects (files) into buckets (directories)
* name of each bucket is globally unique
* buckets are defined at the regional level

##### Naming Convention
* no uppercase
* no underscore
* 3-63 characters long
* not an IP
* starts with lowercase or number

### Objects
* objects have keys: full path to object
  * s3://my-bucket/directory/test.txt
  * prefix (directory/) + object name(test.txt)
* max object size is **5GB**
  * if uploading over the max size, use multi-part upload
* each object has metadata: list of key-value pairs from the system or user defined
* each object has tags: key-value pairs used to categorize

#### Versioning
* versioning is turned on at the bucket level
* new versions don't override previous versions. S3 just creates a copy and increments the versioning count
* helps prevent against unintended deletes
* you can restore to any previous version
* suspending versioning doesn't delete previous versions, just stops keeping track of versions
* objects with no versioning will just have **null** under version

### Encryption
| Encryption Method | Details | Note |
| --- | --- | --- |
| SSE-S3 | encrypt objects using keys managed by AWS |
| SSE-KMS | leverage AWS KMS for encryption keys |
| SSE-C | you manange your own encryption keys | HTTPS ony |
| Client-side Encryption | encrypting message from the client side | AWS stores an already encrypted object

#### SSE-S3
* server-side encryption with AES-256
* set header when uploading: `"x-amz-server-side-encryption": "AES256"`
* after you upload the object to S3, Amazon uses its **S3 managed data key** to store the object with encryption
* key is entirely managed by AWS

#### SSE-KMS
* use AWS Key Management Service to manage encrytion keys
* allows you to give users control over keys and also audit trail of access 
  * good for companies with **rotation policy** of changing keys
* set header when uploading: `"x-amz-server-side-encryption": "aws-kms"`
* after you upload the object to S3, Amazon uses a **KMS key** to store the object with encryption

#### SSE-C
* server-side encryption with key not stored in AWS
* HTTPS only as you must send a encryption key for AWS to encrypt with
* after you upload the object to S3 and provide key, Amazon uses your key to store the object with encryption
* you need to manage your own data key, AWS discards its copy after use

#### Client-side encryption
* encrypt S3 object before uploading to AWS
* AWS stores the encrypted object
* you are responsible for encrypting the object and your own encryption keys yourself

##### Encryption when in-transit
* AWS S3 has endpoints for HTTP and HTTPS
* HTTPS is more secure and recommended
* encryption in flight is called SSL/TLS

### S3 Security
* user based
  * set IAM policies to control user access to S3
* resource based
  * **bucket policies**: set rules on the buckets themselves
  * **object access control list (ACL)**: set permissions on object level
* IAM user can access S3 object if IAM permissions **OR** resource policy allows it
  * **AND** no explicit DENY

#### Bucket Policies
* JSON based policies
* use bucket level policies when
  * granting public access
  * setting objects to be encrypted at upload
  * granting access to another AWS account

##### Bucket Policy Example
```
    {
        "Version": "2020-02-20",
        "Statement": [
            {
                "Sid": "PublicRead",
                "Effect": "Allow",
                "Principal": "*",
                "Action": [
                    "s3:GetObject"
                ],
                "Resource": [
                    "arn..."
                ]
            }
        ]
    }
```

#### Block Public Access
* are ways to block public access through s3 settings
  * access control lists (ACL)
  * explicit toggles in bucket settings
  * can be set at account level

#### Other Security Measures
* setting VPC endpoints for access
* storing access logs
  * create a bucket to store logs of other buckets
  * be aware of logging loops (monitoring a monitor bucket)
    * will create a recursive loop and expand s3 bucket infinitely
* using CloudTrail to monitor API calls
* setting MFA on object delete
  * can only be set through the CLI
  * change if user can delete/turn off versioning on a bucket
* pre-signed URL that expires after a set time

### Cross Origin Resource Sharing
* origin: contains a scheme (protocol), host (domain), port 
```
scheme: HTTPS
host: www.google.com
port: 443
---
https://www.google.com
```
* web browsers only allow requests to other origins if the other origins allow you to
* same origin `http://www.dropbox.com/test1 and http://www.dropbox.com/test2`
* different origin `http://www.example.com and http://other.example.com`
  * cross-origin request (will get blocked unless other origin allows for the request using **CORS headers**)

#### Example of CORS
1. your webserver at http://www.test1.com makes a request to http://www.test2.com by sending a pre-flight request
```
    OPTIONS /
    Host: www.test2.com
    Origin: http://www.test1.com
```
2. the other webserver sends back a response
```
    Access-Control-Allow-Origin: http://www.test1.com
    Access-Control-Allow-Methods: GET, PUT, DELETE
``` 
* other origin specifies what the original origin is allowed to do

#### S3 CORS
* need to enable CORS header if client does a cross-origin request
  * example: html file from one bucket references another file existing on a seperate S3 bucket
    * seperate bucket needs to have CORS enabled in order for our first bucket to access the file
    * CORS enabled not on the requesting bucket, but on the receiving bucket

### S3 Consistency
* modern S3 is highly consistent
* changes made through PUT, overwrites, deletes are immediately reflected upon read and list operations

### S3 Replication
* what if you want to replicate objects in one bucket to a bucket in another region
* you can do **cross-region replication (CRR)** or **same-region replication (SRR)**
* replication is done asynchronously
* after enabling, **only new objects are replicated**
* optional DELETE setting to remove delete markers
  * no delete chaining between multiple buckets

#### Replication Requirements
* source and destination bucket must have versioning enabled
* buckets must belong to the sme account
* s3 must have the correct IAM roles

#### Replication Use Case
* CRR
  * compliance
  * latency access for another region
  * replication (redundancy)
* SRR
  * log aggregation
  * live replication between prod and test

### S3 Storage Classes
| Type | Overview | Use Case
| --- | --- | --- |
| S3 Standard | General purpose storage | big data, content distribution
| S3 Standard - Infrequent Access | Cheaper for less used data | data store for disaster recovery, backups
| S3 One Zone - Infrequent Access | Cheaper but more risky | recreatable data, backup of on-prem data
| S3 Intelligent Tiering | Automatic tiering of storage | unsure what tier to store objects in
| S3 Glacier | Low cost to store, but cost to retrieve | archiving and backup data
| S3 Glacier Deep Archive | Lowest storage cost, highest retrieval cost | data lake

#### S3 Standard
* can sustain two concurrent facility failures
* standard use S3 bucket
* stored in multiple AZs

#### S3 Standard - IA
* used for data data this is **infrequently accessed**, but can be **accessed quickly** when needed

##### S3 Analytics
* can be enabled to help determine when to move objects from Standard to Standard-IA
* creates a daily report
* first report takes 24-48 hours to start
* good for **lifecycle rule** creation

#### S3 One Zone - IA
* stored in a single AZ
* less availability
* lower cost (20% lower than Standard IA)

#### S3 Intelligent Tiering
* pay monthly monitoring and auto tiering fee
* will move your objects between tiers based on usage patterns

#### S3 Glacier Options

##### Glacier Comparison
| | Glacier | Glacier Deep Archive
| --- | --- | --- |
| Expedited Retrieval | 1 to 5 minutes | N/A |
| Standard Retrieval | 2 to 5 hours | 12 hours |
| Bulk | 5 to 12 hours | 48 hourss |
| Minimum Storage Duration | 90 days | 180 days |

##### S3 Glacier 
* alternative for physical magnetic tape
* low cost per storage + retrieval cost
* archives are stored in **vaults**, not buckets

##### S3 Glacier Deep Archive
* used for longer term storage
* high cost for retrieval (more time needed)

### S3 Lifecycle Rules
* transition objects automatically between storage classes
* rules can be set for different scenarios
  * created for certain prefix `s3://my-bucket/personal/*`
  * created for certain tags

#### Transition Actions
* moving objects from one storage class to another
* example: "move objects from Standard to Standard IA 60 days after creation"

#### Expiration Actions
* delete objects after a set amount of time
* examples:
  * deleting old log files
  * removing past versions that aren't needed

### S3 Performance
* S3 can auto-scale to handle high request rates
* application can handle
  * 3500 PUT/COPY/DELETE/POST requests per second per prefix
  * 5500 GET/HEAD requests per second per prefix
* no limits to prefixes in buckets

#### KMS Limitations
* S3 encryptions to KMS impact your KMS API limits
* uploading and downloading all issue seperate API calls to KMS
* counts towards your regional KMS quota
  * 5500, 10 000, 30 000 req/s quota varies based on region
  * if you go over the req/s, you will be throttled

#### Multi-Part Upload
* once all parts are uploaded, S3 will automatically put it back into one file
  
##### S3 Transfer Acceleration
* increase transfer speed by moving file to AWS edge location 
* decreases distance to travel to destination S3 bucket
* can be used for multi-part uploads

#### S3 Byte Range Fetches
* get specific byte ranges to speed up downloads
  * gets can be made in parallel for different byte ranges
* can also be used to only retrieve a specific part of a file 
  * retrieving only the file header

### S3 Select
* retrieve less data when doing SQL using **server-side filtering**
* less data to send means less network transfer and less usage of CPU on client side
* can do basic filtering of rows and columns
* example: getting CSV from S3
  * S3 can filter through the CSV before sending back target data to client

### S3 Requester Pays
* typically bucket owner pays for storage and bucket data transfer cost
* requester pays buckets make it so requester is the one paying for the network/download cost
* requester must be authenticated with AWS so that they can be billed
